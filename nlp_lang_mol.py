# -*- coding: utf-8 -*-
"""NLP_Lang_mol.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U1hP-5EGSvFna1tsKkWjXg6tkvX0qw9g

Team members details:


1.   Amey Sangle
1.   Ananya Sathikumar
2.   Nandan Tandel
2.   Vidhi Tawte
"""

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score

# Load dataset
df = pd.read_csv("employee_perf_dataset.csv")

# Encode labels
label_encoder = LabelEncoder()
df["Output"] = label_encoder.fit_transform(df["Output"])

# Train-validation-test split (80-10-10 split to reduce overfitting)
train_texts, temp_texts, train_labels, temp_labels = train_test_split(
    df["Input"].tolist(), df["Output"].tolist(), test_size=0.2, stratify=df["Output"], random_state=42
)
val_texts, test_texts, val_labels, test_labels = train_test_split(
    temp_texts, temp_labels, test_size=0.5, stratify=temp_labels, random_state=42
)

# Tokenization for BERT and RoBERTa
tokenizer_bert = BertTokenizer.from_pretrained("bert-base-uncased")
tokenizer_roberta = RobertaTokenizer.from_pretrained("roberta-base")

class EmployeeDataset(Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

# Create datasets for BERT and RoBERTa
train_dataset_bert = EmployeeDataset(train_texts, train_labels, tokenizer_bert)
val_dataset_bert = EmployeeDataset(val_texts, val_labels, tokenizer_bert)
test_dataset_bert = EmployeeDataset(test_texts, test_labels, tokenizer_bert)

train_dataset_roberta = EmployeeDataset(train_texts, train_labels, tokenizer_roberta)
val_dataset_roberta = EmployeeDataset(val_texts, val_labels, tokenizer_roberta)
test_dataset_roberta = EmployeeDataset(test_texts, test_labels, tokenizer_roberta)

# Load models
model_bert = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(label_encoder.classes_))
model_roberta = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=len(label_encoder.classes_))

# Training arguments (reduce overfitting with higher weight decay & dropout)
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    save_steps=500,
    weight_decay=0.1,  # Increased weight decay to prevent overfitting
    optim="adamw_torch",
    report_to="none",  # Disables W&B logging
    fp16=True  # Mixed precision for faster training
)

# Trainer for BERT
trainer_bert = Trainer(
    model=model_bert,
    args=training_args,
    train_dataset=train_dataset_bert,
    eval_dataset=val_dataset_bert
)

# Trainer for RoBERTa
trainer_roberta = Trainer(
    model=model_roberta,
    args=training_args,
    train_dataset=train_dataset_roberta,
    eval_dataset=val_dataset_roberta
)

# Train models
print("Training BERT...")
trainer_bert.train()
print("Training RoBERTa...")
trainer_roberta.train()

# Evaluate models
def evaluate_model(trainer, dataset, labels):
    predictions = trainer.predict(dataset).predictions
    predictions = predictions.argmax(axis=1)
    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')
    return accuracy, f1

accuracy_val_bert, f1_val_bert = evaluate_model(trainer_bert, val_dataset_bert, val_labels)
accuracy_test_bert, f1_test_bert = evaluate_model(trainer_bert, test_dataset_bert, test_labels)

accuracy_val_roberta, f1_val_roberta = evaluate_model(trainer_roberta, val_dataset_roberta, val_labels)
accuracy_test_roberta, f1_test_roberta = evaluate_model(trainer_roberta, test_dataset_roberta, test_labels)

print(f"BERT - Validation Accuracy: {accuracy_val_bert:.4f}, F1 Score: {f1_val_bert:.4f}")
print(f"BERT - Test Accuracy: {accuracy_test_bert:.4f}, F1 Score: {f1_test_bert:.4f}")
print(f"RoBERTa - Validation Accuracy: {accuracy_val_roberta:.4f}, F1 Score: {f1_val_roberta:.4f}")
print(f"RoBERTa - Test Accuracy: {accuracy_test_roberta:.4f}, F1 Score: {f1_test_roberta:.4f}")

# Save models
model_bert.save_pretrained("./employee_perf_model_bert")
tokenizer_bert.save_pretrained("./employee_perf_model_bert")

model_roberta.save_pretrained("./employee_perf_model_roberta")
tokenizer_roberta.save_pretrained("./employee_perf_model_roberta")

# Prediction on new input
def predict(text, model, tokenizer):
    encoding = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    output = model(**encoding)
    prediction = torch.argmax(output.logits, dim=1).item()
    return label_encoder.inverse_transform([prediction])[0]

sample_text = "The employee consistently meets deadlines and collaborates well with teammates."

prediction_bert = predict(sample_text, model_bert, tokenizer_bert)
print(f"BERT Predicted Performance: {prediction_bert}")

prediction_roberta = predict(sample_text, model_roberta, tokenizer_roberta)
print(f"RoBERTa Predicted Performance: {prediction_roberta}")

def predict(text, model, tokenizer):
    encoding = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    output = model(**encoding)
    prediction = torch.argmax(output.logits, dim=1).item()
    return label_encoder.inverse_transform([prediction])[0]

sample_text = "The employee consistently meets deadlines and collaborates well with teammates."

prediction_bert = predict(sample_text, model_bert, tokenizer_bert)
print(f"BERT Predicted Performance: {prediction_bert}")

prediction_roberta = predict(sample_text, model_roberta, tokenizer_roberta)
print(f"RoBERTa Predicted Performance: {prediction_roberta}")

