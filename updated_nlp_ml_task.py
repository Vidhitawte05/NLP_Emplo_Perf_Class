# -*- coding: utf-8 -*-
"""Updated_NLP_ML_Task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJYgJ4e9PmHeLMTtcqZeuvz6-WOyyDpT

Team Member Details:


1.   A655 Amey Sangle
1.   A657 Ananya Sathikumar
2.   A663 Nandan Tandel
2.   A665 Vidhi Tawte
"""

from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import numpy as np
import pandas as pd
from scipy.sparse import issparse
from sklearn.preprocessing import MinMaxScaler
import re
import string
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Download NLTK stopwords and wordnet
nltk.download("stopwords")
nltk.download("punkt_tab")
nltk.download("wordnet")

# Load dataset
df = pd.read_csv("test.csv")  # Ensure 'test.csv' contains 'Input' and 'Output' columns

# Display first few rows
print(df.head())

# Check class distribution
plt.figure(figsize=(6, 4))
sns.countplot(x="Output", data=df, palette="viridis")
plt.title("Class Distribution of Employee Performance")
plt.xlabel("Performance Category")
plt.ylabel("Count")
plt.show()

# Initialize tools
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

def preprocess_text(text):
    # Lowercasing
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans("", "", string.punctuation))
    # Remove numbers
    text = re.sub(r"\d+", "", text)
    # Tokenization
    tokens = word_tokenize(text)
    # Remove stopwords and apply lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

# Apply preprocessing to dataset
df["Processed_Input"] = df["Input"].apply(preprocess_text)

# Show sample processed data
print(df[["Input", "Processed_Input"]].head())

# Generate word cloud for most common words
text_combined = " ".join(df["Processed_Input"])

wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text_combined)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Employee Performance Descriptions")
plt.show()

# Function to validate models
def validate_model(model, X_val, y_val):
    y_val_pred = model.predict(X_val)
    print(f"Validation Accuracy: {accuracy_score(y_val, y_val_pred):.4f}")
    print(classification_report(y_val, y_val_pred))

# Load and preprocess data
df = df.drop_duplicates()
print("Class distribution before balancing:\n", df['Output'].value_counts())

# Feature extraction using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=150, min_df=5, ngram_range=(1,2), stop_words='english', norm='l2')
X_tfidf = tfidf_vectorizer.fit_transform(df['Input'])

# Dimensionality reduction using Truncated SVD
svd = TruncatedSVD(n_components=25, random_state=42)
X_reduced = svd.fit_transform(X_tfidf)

# Standardizing the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_reduced)

# Splitting the data (60:20:20)
X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, df['Output'], test_size=0.4, random_state=42, stratify=df['Output'])
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Handling sparse matrices
if issparse(X_train):
    X_train = X_train.toarray()
X_train = np.array(X_train)
y_train = np.array(y_train).ravel()
X_train = np.nan_to_num(X_train)

# Applying SMOTE for class balancing
try:
    smote = SMOTE(sampling_strategy={'Needs Improvement': 350, 'Good': 350, 'Average': 300, 'Excellent': 300}, random_state=42)
    X_train, y_train = smote.fit_resample(X_train, y_train)
    print("Class distribution after SMOTE:\n", pd.Series(y_train).value_counts())
except ValueError as e:
    print("SMOTE Error:", e)

# Defining machine learning models
rf = RandomForestClassifier(n_estimators=15, max_depth=2, max_features=0.15, min_samples_split=6, min_samples_leaf=4, random_state=42, max_samples=0.4)
gb = GradientBoostingClassifier(n_estimators=15, learning_rate=0.002, max_depth=2, min_samples_split=6, min_samples_leaf=4, subsample=0.7, random_state=42, n_iter_no_change=5, validation_fraction=0.2)

gb.fit(X_train, y_train)

# Ensemble model combining Random Forest and Gradient Boosting
ensemble_model = VotingClassifier(estimators=[('rf', rf), ('gb', gb)], voting='hard')

# Dictionary of models
models = {
    "Random Forest": rf,
    "Gradient Boosting": gb,
    "Ensemble Model": ensemble_model
}

# Cross-validation setup
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Training and evaluating models
for name, model in models.items():
    model.fit(X_train, y_train)
    scores = cross_val_score(model, X_train, y_train, cv=skf, scoring="accuracy")
    y_pred = model.predict(X_test)
    print(f"Model: {name}")
    print(f"Cross-Validation Accuracy: {np.mean(scores):.4f} Â± {np.std(scores):.4f}")
    print("Test Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    print("Predicted Class Distribution:", pd.Series(y_pred).value_counts())
    print("="*50)

    # Validate on validation set
    print(f"Validation Results for {name}:")
    validate_model(model, X_val, y_val)

# Function to predict unseen data from Excel
def predict_unseen_data(model, file_path):
    unseen_df = pd.read_excel(file_path)
    unseen_tfidf = tfidf_vectorizer.transform(unseen_df['Input'])
    unseen_reduced = svd.transform(unseen_tfidf)
    unseen_scaled = scaler.transform(unseen_reduced)
    predictions = model.predict(unseen_scaled)
    unseen_df['Predicted Output'] = predictions
    print(unseen_df[['Input', 'Predicted Output']])

predict_unseen_data(rf, 'unseen.xlsx')

# Function to evaluate a single statement using Random Forest
def evaluate_single_statement_rf(statement):
    statement_tfidf = tfidf_vectorizer.transform([statement])
    statement_reduced = svd.transform(statement_tfidf)
    statement_scaled = scaler.transform(statement_reduced)
    prediction = rf.predict(statement_scaled)
    print(f"Predicted Class: {prediction[0]}")

# Example usage:
evaluate_single_statement_rf("The employee frequently misses deadlines and fails to complete tasks on time. Performance has been below expectations with consistent errors in work.Struggles with basic job responsibilities and requires constant supervision.Fails to communicate effectively and often misinterprets instructions. Has received multiple complaints regarding lack of professionalism and commitment.")