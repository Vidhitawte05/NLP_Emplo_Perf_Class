# -*- coding: utf-8 -*-
"""DL_pro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S8xZYXoapu0BwRH-aK5sUAPytzsQByhy

Team Members Details:


1.   A655 Amey Sangle
1.   A657 Ananya Sathikumar
2.   A663 Nandan Tandel
2.   A665 Vidhi Tawte
"""

pip install pandas numpy tensorflow scikit-learn nltk seaborn matplotlib wordcloud joblib

import pandas as pd
import numpy as np
import tensorflow as tf
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Conv1D, GlobalMaxPooling1D
from tensorflow.keras.optimizers import Adam
import joblib

# Download NLTK resources
nltk.download("stopwords")
nltk.download("punkt")
nltk.download("wordnet")

# Load dataset
df = pd.read_csv("test.csv")  # Ensure dataset contains 'Input' and 'Output' columns

# Display first few rows
print(df.head())

# Check class distribution
plt.figure(figsize=(6, 4))
sns.countplot(x="Output", data=df, palette="viridis")
plt.title("Class Distribution of Employee Performance")
plt.xlabel("Performance Category")
plt.ylabel("Count")
plt.show()

# Initialize preprocessing tools
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

def preprocess_text(text):
    tokens = word_tokenize(text.lower())  # Lowercasing & Tokenization
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]  # Lemmatization & Stopword removal
    return " ".join(tokens)

# Apply preprocessing
df["Processed_Input"] = df["Input"].apply(preprocess_text)
print(df[["Input", "Processed_Input"]].head())

# Convert text labels to numeric values
label_map = {label: i for i, label in enumerate(df["Output"].unique())}
df["Encoded_Output"] = df["Output"].map(label_map)

# Split dataset
X = df["Processed_Input"]
y = df["Encoded_Output"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert text into TF-IDF vectors
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train).toarray()
X_test_tfidf = vectorizer.transform(X_test).toarray()

# Save vectorizer
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")

def build_lstm_model(input_dim):
    model = Sequential([
        tf.keras.layers.Input(shape=(input_dim,)),
        Dense(128, activation="relu"),
        Dropout(0.3),
        Dense(64, activation="relu"),
        Dropout(0.3),
        Dense(len(label_map), activation="softmax")  # Output layer
    ])
    model.compile(loss="sparse_categorical_crossentropy", optimizer=Adam(learning_rate=0.001), metrics=["accuracy"])
    return model

def build_bilstm_model(input_dim):
    model = Sequential([
        tf.keras.layers.Input(shape=(input_dim,)),
        Dense(128, activation="relu"),
        Dropout(0.3),
        Dense(64, activation="relu"),
        Dropout(0.3),
        Dense(len(label_map), activation="softmax")  # Output layer
    ])
    model.compile(loss="sparse_categorical_crossentropy", optimizer=Adam(learning_rate=0.001), metrics=["accuracy"])
    return model

def build_cnn_model(input_dim):
    model = Sequential([
        tf.keras.layers.Input(shape=(input_dim,)),
        Dense(128, activation="relu"),
        Dropout(0.3),
        Dense(64, activation="relu"),
        Dropout(0.3),
        Dense(len(label_map), activation="softmax")  # Output layer
    ])
    model.compile(loss="sparse_categorical_crossentropy", optimizer=Adam(learning_rate=0.001), metrics=["accuracy"])
    return model

models = {
    "LSTM": build_lstm_model(X_train_tfidf.shape[1]),
    "BiLSTM": build_bilstm_model(X_train_tfidf.shape[1]),
    "CNN": build_cnn_model(X_train_tfidf.shape[1])
}

best_accuracy = 0
best_model_name = ""
best_model = None

for model_name, model in models.items():
    print(f"\nTraining {model_name} model...\n")
    history = model.fit(X_train_tfidf, y_train, epochs=10, batch_size=16, validation_data=(X_test_tfidf, y_test))

    # Evaluate model
    test_loss, test_accuracy = model.evaluate(X_test_tfidf, y_test)
    print(f"{model_name} Accuracy: {test_accuracy:.4f}")

    # Save best model
    if test_accuracy > best_accuracy:
        best_accuracy = test_accuracy
        best_model_name = model_name
        best_model = model

# Save the best model
best_model.save("best_dl_model.h5")
print(f"\nBest Model: {best_model_name} (Accuracy: {best_accuracy:.4f})")

# Load best model
best_model = tf.keras.models.load_model("best_dl_model.h5")

# Predict on test set
y_pred_probs = best_model.predict(X_test_tfidf)
y_pred = np.argmax(y_pred_probs, axis=1)

# Compute accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Best Model Accuracy: {accuracy:.4f}")

# Classification Report
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=label_map.keys()))

def classify_performance(text):
    # Load model & vectorizer
    vectorizer = joblib.load("tfidf_vectorizer.pkl")
    model = tf.keras.models.load_model("best_dl_model.h5")

    # Preprocess text
    processed_text = preprocess_text(text)
    text_tfidf = vectorizer.transform([processed_text]).toarray()

    # Predict category
    prediction_probs = model.predict(text_tfidf)
    prediction = np.argmax(prediction_probs, axis=1)[0]

    # Convert prediction to label
    category = {v: k for k, v in label_map.items()}[prediction]
    return category

# Example test case
sample_text = "The employee struggles to keep up with deadlines and often submits incomplete work. Their attention to detail is lacking, and their work requires frequent revisions. They are hesitant to ask for help when needed, which leads to confusion and inefficiency. Additionally, they do not contribute actively to team discussions and seem disengaged from company goals."
predicted_label = classify_performance(sample_text)
print(f"Predicted Performance Category: {predicted_label}")

